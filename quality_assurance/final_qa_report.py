#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WabiMail æœ€çµ‚å“è³ªä¿è¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

Task 15: æœ€çµ‚å“è³ªä¿è¨¼ãƒ†ã‚¹ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®å®Œæˆåº¦ã‚’è©•ä¾¡ã—ã€ãƒªãƒªãƒ¼ã‚¹æº–å‚™çŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import os
import sys
import json
import subprocess
import ast
from pathlib import Path
from datetime import datetime
from collections import defaultdict

PROJECT_ROOT = Path(__file__).parent.parent


class WabiMailQualityAssurance:
    """WabiMail å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.qa_start_time = datetime.now()
        self.results = {
            "project_info": {},
            "code_quality": {},
            "documentation": {},
            "build_artifacts": {},
            "test_coverage": {},
            "release_readiness": {}
        }
        
    def log(self, message):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")
    
    def analyze_project_structure(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆ†æ"""
        self.log("ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆ†æ")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ç¢ºèª
        key_directories = [
            "src", "tests", "docs", "resources", "installer", 
            "build_config", "quality_assurance"
        ]
        
        structure_analysis = {}
        total_files = 0
        total_lines = 0
        
        for directory in key_directories:
            dir_path = PROJECT_ROOT / directory
            if dir_path.exists():
                files = list(dir_path.rglob("*.py"))
                file_count = len(files)
                line_count = sum(len(f.read_text(encoding='utf-8', errors='ignore').splitlines()) 
                               for f in files if f.is_file())
                
                structure_analysis[directory] = {
                    "exists": True,
                    "file_count": file_count,
                    "line_count": line_count
                }
                total_files += file_count
                total_lines += line_count
            else:
                structure_analysis[directory] = {"exists": False}
        
        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        critical_files = [
            "src/main.py", "config.yaml", "README.md", "LICENSE",
            "WabiMail.spec", "requirements.txt"
        ]
        
        file_analysis = {}
        for file_path in critical_files:
            full_path = PROJECT_ROOT / file_path
            file_analysis[file_path] = {
                "exists": full_path.exists(),
                "size_kb": round(full_path.stat().st_size / 1024, 2) if full_path.exists() else 0
            }
        
        self.results["project_info"] = {
            "total_files": total_files,
            "total_lines": total_lines,
            "directory_structure": structure_analysis,
            "critical_files": file_analysis,
            "analysis_date": self.qa_start_time.isoformat()
        }
        
        self.log(f"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æå®Œäº†: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«, {total_lines}è¡Œ")
    
    def analyze_code_quality(self):
        """ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ"""
        self.log("ğŸ” ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ")
        
        python_files = list(PROJECT_ROOT.rglob("*.py"))
        
        quality_metrics = {
            "total_python_files": len(python_files),
            "total_lines": 0,
            "total_functions": 0,
            "total_classes": 0,
            "docstring_coverage": 0,
            "complexity_analysis": {},
            "import_analysis": {}
        }
        
        functions_with_docs = 0
        classes_with_docs = 0
        import_counts = defaultdict(int)
        
        for py_file in python_files:
            try:
                content = py_file.read_text(encoding='utf-8')
                lines = content.splitlines()
                quality_metrics["total_lines"] += len(lines)
                
                # ASTè§£æ
                try:
                    tree = ast.parse(content)
                    
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            quality_metrics["total_functions"] += 1
                            if ast.get_docstring(node):
                                functions_with_docs += 1
                        
                        elif isinstance(node, ast.ClassDef):
                            quality_metrics["total_classes"] += 1
                            if ast.get_docstring(node):
                                classes_with_docs += 1
                        
                        elif isinstance(node, (ast.Import, ast.ImportFrom)):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    import_counts[alias.name] += 1
                            else:  # ImportFrom
                                module = node.module or ""
                                import_counts[module] += 1
                
                except SyntaxError:
                    self.log(f"âš ï¸  æ§‹æ–‡ã‚¨ãƒ©ãƒ¼: {py_file}")
                    
            except Exception as e:
                self.log(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {py_file}: {e}")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡è¨ˆç®—
        total_definitions = quality_metrics["total_functions"] + quality_metrics["total_classes"]
        documented_definitions = functions_with_docs + classes_with_docs
        
        if total_definitions > 0:
            quality_metrics["docstring_coverage"] = round(
                (documented_definitions / total_definitions) * 100, 1
            )
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ†æï¼ˆä¸Šä½10ä»¶ï¼‰
        quality_metrics["import_analysis"] = dict(
            sorted(import_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        )
        
        # è¤‡é›‘åº¦åˆ†æï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
        large_files = []
        for py_file in python_files:
            line_count = len(py_file.read_text(encoding='utf-8', errors='ignore').splitlines())
            if line_count > 200:  # 200è¡Œè¶…ã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«
                large_files.append({
                    "file": str(py_file.relative_to(PROJECT_ROOT)),
                    "lines": line_count
                })
        
        quality_metrics["complexity_analysis"] = {
            "large_files": large_files,
            "average_file_size": round(
                quality_metrics["total_lines"] / max(quality_metrics["total_python_files"], 1), 1
            )
        }
        
        self.results["code_quality"] = quality_metrics
        
        self.log(f"âœ… ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æå®Œäº†:")
        self.log(f"   ğŸ“Š {quality_metrics['total_python_files']}ãƒ•ã‚¡ã‚¤ãƒ«, {quality_metrics['total_lines']}è¡Œ")
        self.log(f"   ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡: {quality_metrics['docstring_coverage']}%")
        self.log(f"   ğŸ”§ {quality_metrics['total_functions']}é–¢æ•°, {quality_metrics['total_classes']}ã‚¯ãƒ©ã‚¹")
    
    def analyze_documentation(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æ"""
        self.log("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æ")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        doc_files = {
            "README.md": PROJECT_ROOT / "README.md",
            "LICENSE": PROJECT_ROOT / "LICENSE", 
            "config.yaml": PROJECT_ROOT / "config.yaml",
            "requirements.txt": PROJECT_ROOT / "requirements.txt",
            "build_instructions.md": PROJECT_ROOT / "docs" / "build_instructions.md",
            "installer_README.md": PROJECT_ROOT / "installer" / "README.md"
        }
        
        doc_analysis = {}
        total_doc_size = 0
        
        for doc_name, doc_path in doc_files.items():
            if doc_path.exists():
                content = doc_path.read_text(encoding='utf-8', errors='ignore')
                lines = len(content.splitlines())
                size_kb = round(doc_path.stat().st_size / 1024, 2)
                
                doc_analysis[doc_name] = {
                    "exists": True,
                    "lines": lines,
                    "size_kb": size_kb,
                    "char_count": len(content)
                }
                total_doc_size += size_kb
            else:
                doc_analysis[doc_name] = {"exists": False}
        
        # é–‹ç™ºãƒãƒ¼ãƒˆåˆ†æ
        note_dir = PROJECT_ROOT / "docs" / "note" / "development"
        blog_dir = PROJECT_ROOT / "docs" / "note" / "blog"
        
        development_notes = len(list(note_dir.glob("*.md"))) if note_dir.exists() else 0
        blog_posts = len(list(blog_dir.glob("*.md"))) if blog_dir.exists() else 0
        
        self.results["documentation"] = {
            "core_documents": doc_analysis,
            "total_doc_size_kb": round(total_doc_size, 2),
            "development_notes": development_notes,
            "blog_posts": blog_posts,
            "documentation_completeness": self._calculate_doc_completeness(doc_analysis)
        }
        
        self.log(f"âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æå®Œäº†:")
        self.log(f"   ğŸ“– ã‚³ã‚¢ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {sum(1 for d in doc_analysis.values() if d.get('exists', False))}/{len(doc_analysis)}")
        self.log(f"   ğŸ“ é–‹ç™ºãƒãƒ¼ãƒˆ: {development_notes}ä»¶")
        self.log(f"   ğŸ“° ãƒ–ãƒ­ã‚°è¨˜äº‹: {blog_posts}ä»¶")
    
    def _calculate_doc_completeness(self, doc_analysis):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œæˆåº¦è¨ˆç®—"""
        required_docs = ["README.md", "LICENSE", "config.yaml"]
        existing_required = sum(1 for doc in required_docs if doc_analysis.get(doc, {}).get('exists', False))
        
        optional_docs = ["requirements.txt", "build_instructions.md", "installer_README.md"]
        existing_optional = sum(1 for doc in optional_docs if doc_analysis.get(doc, {}).get('exists', False))
        
        # å¿…é ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ100% + ã‚ªãƒ—ã‚·ãƒ§ãƒ³50%é‡ã¿
        total_score = (existing_required / len(required_docs)) * 100
        optional_score = (existing_optional / len(optional_docs)) * 50
        
        return round(min(total_score + optional_score, 100), 1)
    
    def analyze_build_artifacts(self):
        """ãƒ“ãƒ«ãƒ‰æˆæœç‰©åˆ†æ"""
        self.log("ğŸ”¨ ãƒ“ãƒ«ãƒ‰æˆæœç‰©åˆ†æ")
        
        artifacts = {}
        
        # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        exe_files = list(PROJECT_ROOT.glob("dist/*"))
        if exe_files:
            artifacts["executables"] = []
            for exe_file in exe_files:
                if exe_file.is_file():
                    size_mb = round(exe_file.stat().st_size / (1024 * 1024), 2)
                    artifacts["executables"].append({
                        "name": exe_file.name,
                        "size_mb": size_mb,
                        "path": str(exe_file.relative_to(PROJECT_ROOT))
                    })
        
        # PyInstaller Spec ãƒ•ã‚¡ã‚¤ãƒ«
        spec_file = PROJECT_ROOT / "WabiMail.spec"
        artifacts["pyinstaller_spec"] = {
            "exists": spec_file.exists(),
            "size_kb": round(spec_file.stat().st_size / 1024, 2) if spec_file.exists() else 0
        }
        
        # Inno Setup ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        iss_file = PROJECT_ROOT / "installer" / "wabimail_installer.iss"
        artifacts["inno_setup_script"] = {
            "exists": iss_file.exists(),
            "size_kb": round(iss_file.stat().st_size / 1024, 2) if iss_file.exists() else 0
        }
        
        # ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        build_scripts = [
            "build_simple.py", "build_exe.py", "installer/build_installer.py"
        ]
        
        artifacts["build_scripts"] = {}
        for script in build_scripts:
            script_path = PROJECT_ROOT / script
            artifacts["build_scripts"][script] = {
                "exists": script_path.exists(),
                "size_kb": round(script_path.stat().st_size / 1024, 2) if script_path.exists() else 0
            }
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        test_scripts = [
            "test_executable.py", "installer/test_installer.py", "tests/test_integration.py"
        ]
        
        artifacts["test_scripts"] = {}
        for script in test_scripts:
            script_path = PROJECT_ROOT / script
            artifacts["test_scripts"][script] = {
                "exists": script_path.exists(),
                "size_kb": round(script_path.stat().st_size / 1024, 2) if script_path.exists() else 0
            }
        
        self.results["build_artifacts"] = artifacts
        
        # çµæœè¡¨ç¤º
        exe_count = len(artifacts.get("executables", []))
        build_script_count = sum(1 for s in artifacts["build_scripts"].values() if s["exists"])
        test_script_count = sum(1 for s in artifacts["test_scripts"].values() if s["exists"])
        
        self.log(f"âœ… ãƒ“ãƒ«ãƒ‰æˆæœç‰©åˆ†æå®Œäº†:")
        self.log(f"   ğŸš€ å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«: {exe_count}å€‹")
        self.log(f"   ğŸ”§ ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {build_script_count}/{len(build_scripts)}å€‹")
        self.log(f"   ğŸ§ª ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {test_script_count}/{len(test_scripts)}å€‹")
    
    def analyze_test_coverage(self):
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
        self.log("ğŸ§ª ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ")
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        test_files = list(PROJECT_ROOT.rglob("test_*.py")) + list(PROJECT_ROOT.rglob("*_test.py"))
        
        test_analysis = {
            "test_file_count": len(test_files),
            "test_categories": {},
            "integration_tests": False,
            "unit_tests": False,
            "build_tests": False,
            "installer_tests": False
        }
        
        for test_file in test_files:
            category = "other"
            if "integration" in test_file.name:
                category = "integration"
                test_analysis["integration_tests"] = True
            elif "unit" in test_file.name:
                category = "unit"
                test_analysis["unit_tests"] = True
            elif "executable" in test_file.name or "build" in test_file.name:
                category = "build"
                test_analysis["build_tests"] = True
            elif "installer" in test_file.name:
                category = "installer"
                test_analysis["installer_tests"] = True
            
            if category not in test_analysis["test_categories"]:
                test_analysis["test_categories"][category] = []
            
            test_analysis["test_categories"][category].append({
                "file": str(test_file.relative_to(PROJECT_ROOT)),
                "size_kb": round(test_file.stat().st_size / 1024, 2)
            })
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸æ¨å®š
        src_files = len(list((PROJECT_ROOT / "src").rglob("*.py")))
        test_to_src_ratio = round(len(test_files) / max(src_files, 1), 2)
        
        test_analysis["coverage_estimation"] = {
            "test_to_source_ratio": test_to_src_ratio,
            "estimated_coverage": min(test_to_src_ratio * 100, 100)
        }
        
        self.results["test_coverage"] = test_analysis
        
        self.log(f"âœ… ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æå®Œäº†:")
        self.log(f"   ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {len(test_files)}å€‹")
        self.log(f"   ğŸ“Š æ¨å®šã‚«ãƒãƒ¬ãƒƒã‚¸: {test_analysis['coverage_estimation']['estimated_coverage']:.1f}%")
        
        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—è¡¨ç¤º
        test_types = [
            ("çµ±åˆãƒ†ã‚¹ãƒˆ", test_analysis["integration_tests"]),
            ("ãƒ“ãƒ«ãƒ‰ãƒ†ã‚¹ãƒˆ", test_analysis["build_tests"]),
            ("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ", test_analysis["installer_tests"])
        ]
        
        for test_type, exists in test_types:
            status = "âœ…" if exists else "âŒ"
            self.log(f"   {status} {test_type}")
    
    def evaluate_release_readiness(self):
        """ãƒªãƒªãƒ¼ã‚¹æº–å‚™çŠ¶æ³è©•ä¾¡"""
        self.log("ğŸš€ ãƒªãƒªãƒ¼ã‚¹æº–å‚™çŠ¶æ³è©•ä¾¡")
        
        readiness_criteria = {
            "core_functionality": self._check_core_functionality(),
            "documentation_quality": self._check_documentation_quality(),
            "build_system": self._check_build_system(),
            "test_coverage": self._check_test_coverage(),
            "code_quality": self._check_code_quality(),
            "deployment_readiness": self._check_deployment_readiness()
        }
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = [criteria["score"] for criteria in readiness_criteria.values()]
        overall_score = round(sum(scores) / len(scores), 1)
        
        # ãƒªãƒªãƒ¼ã‚¹æº–å‚™ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if overall_score >= 90:
            readiness_level = "READY"
            readiness_message = "ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†"
        elif overall_score >= 80:
            readiness_level = "ALMOST_READY"
            readiness_message = "è»½å¾®ãªä¿®æ­£ãŒå¿…è¦"
        elif overall_score >= 70:
            readiness_level = "NEEDS_WORK"
            readiness_message = "è¿½åŠ ä½œæ¥­ãŒå¿…è¦"
        else:
            readiness_level = "NOT_READY"
            readiness_message = "å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦"
        
        self.results["release_readiness"] = {
            "overall_score": overall_score,
            "readiness_level": readiness_level,
            "readiness_message": readiness_message,
            "criteria": readiness_criteria,
            "recommendations": self._generate_recommendations(readiness_criteria)
        }
        
        self.log(f"âœ… ãƒªãƒªãƒ¼ã‚¹æº–å‚™è©•ä¾¡å®Œäº†:")
        self.log(f"   ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {overall_score}/100")
        self.log(f"   ğŸ¯ æº–å‚™çŠ¶æ³: {readiness_message}")
    
    def _check_core_functionality(self):
        """ã‚³ã‚¢æ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯"""
        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        critical_files = [
            "src/main.py", "src/ui/main_window.py", "src/mail/imap_client.py",
            "src/mail/smtp_client.py", "src/auth/gmail_oauth.py"
        ]
        
        existing_files = sum(1 for f in critical_files if (PROJECT_ROOT / f).exists())
        score = round((existing_files / len(critical_files)) * 100, 1)
        
        return {
            "score": score,
            "details": f"{existing_files}/{len(critical_files)} ã‚³ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨",
            "status": "GOOD" if score >= 80 else "NEEDS_IMPROVEMENT"
        }
    
    def _check_documentation_quality(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ªãƒã‚§ãƒƒã‚¯"""
        doc_completeness = self.results["documentation"]["documentation_completeness"]
        
        return {
            "score": doc_completeness,
            "details": f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œæˆåº¦ {doc_completeness}%",
            "status": "GOOD" if doc_completeness >= 80 else "NEEDS_IMPROVEMENT"
        }
    
    def _check_build_system(self):
        """ãƒ“ãƒ«ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯"""
        required_build_files = [
            "WabiMail.spec", "build_simple.py", "build_exe.py", 
            "installer/wabimail_installer.iss", "installer/build_installer.py"
        ]
        
        existing_build_files = sum(1 for f in required_build_files if (PROJECT_ROOT / f).exists())
        score = round((existing_build_files / len(required_build_files)) * 100, 1)
        
        return {
            "score": score,
            "details": f"{existing_build_files}/{len(required_build_files)} ãƒ“ãƒ«ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨",
            "status": "GOOD" if score >= 80 else "NEEDS_IMPROVEMENT"
        }
    
    def _check_test_coverage(self):
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
        test_coverage = self.results["test_coverage"]
        
        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã®å­˜åœ¨ç¢ºèª
        test_types = [
            test_coverage["integration_tests"],
            test_coverage["build_tests"],
            test_coverage["installer_tests"]
        ]
        
        existing_test_types = sum(1 for t in test_types if t)
        score = round((existing_test_types / len(test_types)) * 100, 1)
        
        return {
            "score": score,
            "details": f"{existing_test_types}/{len(test_types)} ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—å®Ÿè£…",
            "status": "GOOD" if score >= 66 else "NEEDS_IMPROVEMENT"
        }
    
    def _check_code_quality(self):
        """ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯"""
        code_quality = self.results["code_quality"]
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‹ã‚‰å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
        doc_coverage = code_quality["docstring_coverage"]
        avg_file_size = code_quality["complexity_analysis"]["average_file_size"]
        
        # é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆ50-300è¡Œï¼‰
        size_score = 100 if 50 <= avg_file_size <= 300 else max(0, 100 - abs(avg_file_size - 175))
        
        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        score = round((doc_coverage + size_score) / 2, 1)
        
        return {
            "score": score,
            "details": f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡{doc_coverage}%, å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º{avg_file_size}è¡Œ",
            "status": "GOOD" if score >= 70 else "NEEDS_IMPROVEMENT"
        }
    
    def _check_deployment_readiness(self):
        """ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™ãƒã‚§ãƒƒã‚¯"""
        # é…å¸ƒã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        deployment_files = [
            "README.md", "LICENSE", "config.yaml"
        ]
        
        existing_deployment_files = sum(1 for f in deployment_files if (PROJECT_ROOT / f).exists())
        
        # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        has_executable = len(self.results["build_artifacts"].get("executables", [])) > 0
        
        # ã‚¹ã‚³ã‚¢ç®—å‡º
        file_score = (existing_deployment_files / len(deployment_files)) * 70
        exe_score = 30 if has_executable else 0
        score = round(file_score + exe_score, 1)
        
        return {
            "score": score,
            "details": f"{existing_deployment_files}/{len(deployment_files)} é…å¸ƒãƒ•ã‚¡ã‚¤ãƒ«, å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«{'ã‚ã‚Š' if has_executable else 'ãªã—'}",
            "status": "GOOD" if score >= 80 else "NEEDS_IMPROVEMENT"
        }
    
    def _generate_recommendations(self, criteria):
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        recommendations = []
        
        for criterion_name, criterion_data in criteria.items():
            if criterion_data["status"] == "NEEDS_IMPROVEMENT":
                if criterion_name == "core_functionality":
                    recommendations.append("ä¸è¶³ã—ã¦ã„ã‚‹ã‚³ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„")
                elif criterion_name == "documentation_quality":
                    recommendations.append("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å……å®Ÿã‚’å›³ã£ã¦ãã ã•ã„")
                elif criterion_name == "build_system":
                    recommendations.append("ãƒ“ãƒ«ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Œæˆã•ã›ã¦ãã ã•ã„")
                elif criterion_name == "test_coverage":
                    recommendations.append("ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„")
                elif criterion_name == "code_quality":
                    recommendations.append("ã‚³ãƒ¼ãƒ‰å“è³ªï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€æ§‹é€ ï¼‰ã‚’æ”¹å–„ã—ã¦ãã ã•ã„")
                elif criterion_name == "deployment_readiness":
                    recommendations.append("é…å¸ƒæº–å‚™ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´å‚™ã—ã¦ãã ã•ã„")
        
        if not recommendations:
            recommendations.append("ã™ã¹ã¦ã®åŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™ã€‚ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†ã§ã™ï¼")
        
        return recommendations
    
    def generate_comprehensive_report(self):
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        qa_end_time = datetime.now()
        qa_duration = qa_end_time - self.qa_start_time
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
        final_report = {
            "report_info": {
                "generated_at": qa_end_time.isoformat(),
                "analysis_duration_seconds": qa_duration.total_seconds(),
                "wabimail_version": "1.0.0",
                "analysis_tool": "WabiMail Quality Assurance System"
            },
            **self.results
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_dir = PROJECT_ROOT / "quality_assurance" / "reports"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = self.qa_start_time.strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"final_qa_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚µãƒãƒªãƒ¼å‡ºåŠ›
        self._print_summary_report(final_report)
        
        self.log(f"ğŸ“„ æœ€çµ‚å“è³ªä¿è¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        return report_file
    
    def _print_summary_report(self, report):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print("\n" + "ğŸŒ¸" * 30)
        print("WabiMail æœ€çµ‚å“è³ªä¿è¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        print("ğŸŒ¸" * 30)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
        project_info = report["project_info"]
        print(f"\nğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:")
        print(f"  ãƒ»ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {project_info['total_files']:,}")
        print(f"  ãƒ»ç·è¡Œæ•°: {project_info['total_lines']:,}")
        
        # ã‚³ãƒ¼ãƒ‰å“è³ª
        code_quality = report["code_quality"]
        print(f"\nğŸ” ã‚³ãƒ¼ãƒ‰å“è³ª:")
        print(f"  ãƒ»Pythonãƒ•ã‚¡ã‚¤ãƒ«: {code_quality['total_python_files']}")
        print(f"  ãƒ»é–¢æ•°æ•°: {code_quality['total_functions']}")
        print(f"  ãƒ»ã‚¯ãƒ©ã‚¹æ•°: {code_quality['total_classes']}")
        print(f"  ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡: {code_quality['docstring_coverage']}%")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        documentation = report["documentation"]
        print(f"\nğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:")
        print(f"  ãƒ»å®Œæˆåº¦: {documentation['documentation_completeness']}%")
        print(f"  ãƒ»é–‹ç™ºãƒãƒ¼ãƒˆ: {documentation['development_notes']}ä»¶")
        print(f"  ãƒ»ãƒ–ãƒ­ã‚°è¨˜äº‹: {documentation['blog_posts']}ä»¶")
        
        # ãƒ“ãƒ«ãƒ‰æˆæœç‰©
        build_artifacts = report["build_artifacts"]
        exe_count = len(build_artifacts.get("executables", []))
        print(f"\nğŸ”¨ ãƒ“ãƒ«ãƒ‰æˆæœç‰©:")
        print(f"  ãƒ»å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«: {exe_count}å€‹")
        print(f"  ãƒ»PyInstaller Spec: {'âœ…' if build_artifacts['pyinstaller_spec']['exists'] else 'âŒ'}")
        print(f"  ãƒ»Inno Setup: {'âœ…' if build_artifacts['inno_setup_script']['exists'] else 'âŒ'}")
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
        test_coverage = report["test_coverage"]
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸:")
        print(f"  ãƒ»ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {test_coverage['test_file_count']}å€‹")
        print(f"  ãƒ»çµ±åˆãƒ†ã‚¹ãƒˆ: {'âœ…' if test_coverage['integration_tests'] else 'âŒ'}")
        print(f"  ãƒ»ãƒ“ãƒ«ãƒ‰ãƒ†ã‚¹ãƒˆ: {'âœ…' if test_coverage['build_tests'] else 'âŒ'}")
        print(f"  ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ: {'âœ…' if test_coverage['installer_tests'] else 'âŒ'}")
        
        # ãƒªãƒªãƒ¼ã‚¹æº–å‚™çŠ¶æ³
        release_readiness = report["release_readiness"]
        print(f"\nğŸš€ ãƒªãƒªãƒ¼ã‚¹æº–å‚™çŠ¶æ³:")
        print(f"  ãƒ»ç·åˆã‚¹ã‚³ã‚¢: {release_readiness['overall_score']}/100")
        print(f"  ãƒ»æº–å‚™ãƒ¬ãƒ™ãƒ«: {release_readiness['readiness_message']}")
        
        # æ”¹å–„ææ¡ˆ
        if release_readiness["recommendations"]:
            print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
            for i, rec in enumerate(release_readiness["recommendations"], 1):
                print(f"  {i}. {rec}")
        
        print("\n" + "ğŸŒ¸" * 30)
        print("ä¾˜ã³å¯‚ã³ã®ç¾å­¦ã«åŸºã¥ãå“è³ªä¿è¨¼å®Œäº†")
        print("ğŸŒ¸" * 30 + "\n")
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å“è³ªä¿è¨¼å‡¦ç†"""
        print("ğŸŒ¸ WabiMail æœ€çµ‚å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 60)
        print(f"åˆ†æé–‹å§‹: {self.qa_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        try:
            # 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆ†æ
            self.analyze_project_structure()
            
            # 2. ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ
            self.analyze_code_quality()
            
            # 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æ
            self.analyze_documentation()
            
            # 4. ãƒ“ãƒ«ãƒ‰æˆæœç‰©åˆ†æ
            self.analyze_build_artifacts()
            
            # 5. ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
            self.analyze_test_coverage()
            
            # 6. ãƒªãƒªãƒ¼ã‚¹æº–å‚™çŠ¶æ³è©•ä¾¡
            self.evaluate_release_readiness()
            
            # 7. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_file = self.generate_comprehensive_report()
            
            return True
            
        except Exception as e:
            self.log(f"âŒ å“è³ªä¿è¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    qa_system = WabiMailQualityAssurance()
    
    try:
        success = qa_system.run()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâŒ å“è³ªä¿è¨¼ãƒ—ãƒ­ã‚»ã‚¹ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()